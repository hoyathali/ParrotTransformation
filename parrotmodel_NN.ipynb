{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hoyathali/ParrotTransformation/blob/main/parrotmodel_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cA7Bcrc0UqVC",
        "outputId": "bc376f6d-30eb-45a1-921b-de09c2d786ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUSwkELh3T6P",
        "outputId": "1a86eaba-de9f-4c8d-986a-cceb0bbc960f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "1_LdZhoWUqVE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zw4_NQ7oUqVF",
        "outputId": "6305b73a-f532-47fe-847b-4eadfbe4f5b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-446f0e3dcfac>:13: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data=data.append(transposed_data2)\n",
            "<ipython-input-3-446f0e3dcfac>:14: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  data=data.append(transposed_data)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7417, 16384)\n"
          ]
        }
      ],
      "source": [
        "raw_data_path='output_backup.csv'\n",
        "df= pd.read_csv(raw_data_path,header=None)\n",
        "transposed_data=df.transpose()\n",
        "transposed_data = transposed_data.iloc[:, 1:]\n",
        "\n",
        "raw_data_path2='output2.csv'\n",
        "df2= pd.read_csv(raw_data_path2,header=None)\n",
        "transposed_data2=df2.transpose()\n",
        "transposed_data2 = transposed_data2.iloc[:, 1:]\n",
        "\n",
        "data = pd.DataFrame()\n",
        "\n",
        "data=data.append(transposed_data2)\n",
        "data=data.append(transposed_data)\n",
        "\n",
        "print(data.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1lpL-S6UqVF",
        "outputId": "a7ab3b7d-6724-4136-99a3-8d8113cfabfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5847, 12288)\n",
            "(5847, 4096)\n"
          ]
        }
      ],
      "source": [
        "no_of_features=64*64*3\n",
        "no_of_outputs=64*64\n",
        "\n",
        "X=transposed_data.iloc[:,:no_of_features]\n",
        "y=transposed_data.iloc[:, -no_of_outputs:]\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "tHAatU21UqVG"
      },
      "outputs": [],
      "source": [
        "# Step 1: Split the data into training, validation, and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Step 2: Normalize the data\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler on the training data and transform both training and validation data\n",
        "X_train_normalized = scaler.fit_transform(X_train)\n",
        "X_val_normalized = scaler.transform(X_val)\n",
        "X_test_normalized = scaler.transform(X_test)\n",
        "\n",
        "# Convert normalized data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_normalized, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)  # Assuming labels are floats\n",
        "\n",
        "X_val_tensor = torch.tensor(X_val_normalized, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test_normalized, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
        "\n",
        "# Step 3: Create PyTorch datasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "177j79kGUqVG"
      },
      "outputs": [],
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        hidden_size = (input_size // 3) * 2\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size//2)\n",
        "        self.fc3 = nn.Linear(hidden_size//2, output_size)\n",
        "\n",
        "\n",
        "\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.zeros_(self.fc1.bias)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.zeros_(self.fc2.bias)\n",
        "        nn.init.xavier_uniform_(self.fc3.weight)\n",
        "        nn.init.zeros_(self.fc3.bias)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), negative_slope=0.01)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        #x2=x+x2\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "input_size = 64 * 64 * 3\n",
        "output_size = 64 * 64\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "tNDb6uKqUqVH"
      },
      "outputs": [],
      "source": [
        "batch_size = 16  # Adjust the batch size based on your needs\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "FNoQ0PnCUqVH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed636fb-5342-41bc-cd55-6b91d4074ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Epoch [1/200], Average Loss: 4765.2219\n",
            "Validation Epoch [1/200], Average Loss: 2284.7819\n",
            "Training Epoch [2/200], Average Loss: 1881.4145\n",
            "Validation Epoch [2/200], Average Loss: 1736.6194\n",
            "Training Epoch [3/200], Average Loss: 1517.4153\n",
            "Validation Epoch [3/200], Average Loss: 1517.1604\n",
            "Training Epoch [4/200], Average Loss: 1325.0945\n",
            "Validation Epoch [4/200], Average Loss: 1382.7034\n",
            "Training Epoch [5/200], Average Loss: 1179.6344\n",
            "Validation Epoch [5/200], Average Loss: 1263.2250\n",
            "Training Epoch [6/200], Average Loss: 1083.2573\n",
            "Validation Epoch [6/200], Average Loss: 1154.1198\n",
            "Training Epoch [7/200], Average Loss: 978.9878\n",
            "Validation Epoch [7/200], Average Loss: 1092.4972\n",
            "Training Epoch [8/200], Average Loss: 907.2826\n",
            "Validation Epoch [8/200], Average Loss: 1017.8893\n",
            "Training Epoch [9/200], Average Loss: 841.3647\n",
            "Validation Epoch [9/200], Average Loss: 971.1115\n",
            "Training Epoch [10/200], Average Loss: 784.9848\n",
            "Validation Epoch [10/200], Average Loss: 926.4671\n",
            "Training Epoch [11/200], Average Loss: 743.7676\n",
            "Validation Epoch [11/200], Average Loss: 882.5325\n",
            "Training Epoch [12/200], Average Loss: 708.4966\n",
            "Validation Epoch [12/200], Average Loss: 842.3425\n",
            "Training Epoch [13/200], Average Loss: 668.1632\n",
            "Validation Epoch [13/200], Average Loss: 817.6527\n",
            "Training Epoch [14/200], Average Loss: 638.6279\n",
            "Validation Epoch [14/200], Average Loss: 790.7871\n",
            "Training Epoch [15/200], Average Loss: 611.2999\n",
            "Validation Epoch [15/200], Average Loss: 762.4268\n",
            "Training Epoch [16/200], Average Loss: 588.2291\n",
            "Validation Epoch [16/200], Average Loss: 741.2616\n",
            "Training Epoch [17/200], Average Loss: 563.6084\n",
            "Validation Epoch [17/200], Average Loss: 720.2271\n",
            "Training Epoch [18/200], Average Loss: 547.2434\n",
            "Validation Epoch [18/200], Average Loss: 706.0591\n",
            "Training Epoch [19/200], Average Loss: 522.3754\n",
            "Validation Epoch [19/200], Average Loss: 687.0979\n",
            "Training Epoch [20/200], Average Loss: 504.0951\n",
            "Validation Epoch [20/200], Average Loss: 667.0927\n",
            "Training Epoch [21/200], Average Loss: 487.1228\n",
            "Validation Epoch [21/200], Average Loss: 652.2900\n",
            "Training Epoch [22/200], Average Loss: 473.5325\n",
            "Validation Epoch [22/200], Average Loss: 639.3243\n",
            "Training Epoch [23/200], Average Loss: 456.8389\n",
            "Validation Epoch [23/200], Average Loss: 626.2738\n",
            "Training Epoch [24/200], Average Loss: 442.0584\n",
            "Validation Epoch [24/200], Average Loss: 613.0517\n",
            "Training Epoch [25/200], Average Loss: 429.2538\n",
            "Validation Epoch [25/200], Average Loss: 599.3530\n",
            "Training Epoch [26/200], Average Loss: 418.0225\n",
            "Validation Epoch [26/200], Average Loss: 597.0476\n",
            "Training Epoch [27/200], Average Loss: 409.9773\n",
            "Validation Epoch [27/200], Average Loss: 576.5019\n",
            "Training Epoch [28/200], Average Loss: 395.9870\n",
            "Validation Epoch [28/200], Average Loss: 568.7321\n",
            "Training Epoch [29/200], Average Loss: 385.1331\n",
            "Validation Epoch [29/200], Average Loss: 557.5598\n",
            "Training Epoch [30/200], Average Loss: 374.2491\n",
            "Validation Epoch [30/200], Average Loss: 545.9747\n",
            "Training Epoch [31/200], Average Loss: 364.3472\n",
            "Validation Epoch [31/200], Average Loss: 540.0174\n",
            "Training Epoch [32/200], Average Loss: 354.5240\n",
            "Validation Epoch [32/200], Average Loss: 529.7954\n",
            "Training Epoch [33/200], Average Loss: 345.1802\n",
            "Validation Epoch [33/200], Average Loss: 523.0992\n",
            "Training Epoch [34/200], Average Loss: 336.9077\n",
            "Validation Epoch [34/200], Average Loss: 512.6829\n",
            "Training Epoch [35/200], Average Loss: 328.5786\n",
            "Validation Epoch [35/200], Average Loss: 505.0636\n",
            "Training Epoch [36/200], Average Loss: 320.8059\n",
            "Validation Epoch [36/200], Average Loss: 500.6070\n",
            "Training Epoch [37/200], Average Loss: 313.2097\n",
            "Validation Epoch [37/200], Average Loss: 492.2246\n",
            "Training Epoch [38/200], Average Loss: 305.9766\n",
            "Validation Epoch [38/200], Average Loss: 486.6263\n",
            "Training Epoch [39/200], Average Loss: 299.7259\n",
            "Validation Epoch [39/200], Average Loss: 479.4707\n",
            "Training Epoch [40/200], Average Loss: 293.0426\n",
            "Validation Epoch [40/200], Average Loss: 474.6392\n",
            "Training Epoch [41/200], Average Loss: 286.6627\n",
            "Validation Epoch [41/200], Average Loss: 469.3262\n",
            "Training Epoch [42/200], Average Loss: 279.9288\n",
            "Validation Epoch [42/200], Average Loss: 462.0586\n",
            "Training Epoch [43/200], Average Loss: 274.0801\n",
            "Validation Epoch [43/200], Average Loss: 456.2697\n",
            "Training Epoch [44/200], Average Loss: 268.2462\n",
            "Validation Epoch [44/200], Average Loss: 451.7998\n",
            "Training Epoch [45/200], Average Loss: 262.9086\n",
            "Validation Epoch [45/200], Average Loss: 446.2581\n",
            "Training Epoch [46/200], Average Loss: 259.7990\n",
            "Validation Epoch [46/200], Average Loss: 446.6260\n",
            "Validation loss did not improve. Patience left: 4/5\n",
            "Training Epoch [47/200], Average Loss: 252.9306\n",
            "Validation Epoch [47/200], Average Loss: 437.4011\n",
            "Training Epoch [48/200], Average Loss: 248.0278\n",
            "Validation Epoch [48/200], Average Loss: 434.8968\n",
            "Training Epoch [49/200], Average Loss: 243.2972\n",
            "Validation Epoch [49/200], Average Loss: 428.1180\n",
            "Training Epoch [50/200], Average Loss: 238.2993\n",
            "Validation Epoch [50/200], Average Loss: 424.8029\n",
            "Training Epoch [51/200], Average Loss: 233.6545\n",
            "Validation Epoch [51/200], Average Loss: 420.7496\n",
            "Training Epoch [52/200], Average Loss: 228.7182\n",
            "Validation Epoch [52/200], Average Loss: 416.1937\n",
            "Training Epoch [53/200], Average Loss: 224.4580\n",
            "Validation Epoch [53/200], Average Loss: 411.6066\n",
            "Training Epoch [54/200], Average Loss: 220.1790\n",
            "Validation Epoch [54/200], Average Loss: 407.3334\n",
            "Training Epoch [55/200], Average Loss: 215.7289\n",
            "Validation Epoch [55/200], Average Loss: 404.6575\n",
            "Training Epoch [56/200], Average Loss: 212.1656\n",
            "Validation Epoch [56/200], Average Loss: 402.0784\n",
            "Training Epoch [57/200], Average Loss: 208.5570\n",
            "Validation Epoch [57/200], Average Loss: 398.7705\n",
            "Training Epoch [58/200], Average Loss: 204.8961\n",
            "Validation Epoch [58/200], Average Loss: 394.3293\n",
            "Training Epoch [59/200], Average Loss: 201.0658\n",
            "Validation Epoch [59/200], Average Loss: 393.1360\n",
            "Training Epoch [60/200], Average Loss: 197.6509\n",
            "Validation Epoch [60/200], Average Loss: 391.3776\n",
            "Training Epoch [61/200], Average Loss: 194.4143\n",
            "Validation Epoch [61/200], Average Loss: 385.9993\n",
            "Training Epoch [62/200], Average Loss: 190.9585\n",
            "Validation Epoch [62/200], Average Loss: 384.3316\n",
            "Training Epoch [63/200], Average Loss: 188.1264\n",
            "Validation Epoch [63/200], Average Loss: 379.8518\n",
            "Training Epoch [64/200], Average Loss: 184.2295\n",
            "Validation Epoch [64/200], Average Loss: 377.4609\n",
            "Training Epoch [65/200], Average Loss: 181.4056\n",
            "Validation Epoch [65/200], Average Loss: 374.4631\n",
            "Training Epoch [66/200], Average Loss: 178.0038\n",
            "Validation Epoch [66/200], Average Loss: 370.5567\n",
            "Training Epoch [67/200], Average Loss: 174.8063\n",
            "Validation Epoch [67/200], Average Loss: 369.0428\n",
            "Training Epoch [68/200], Average Loss: 172.4759\n",
            "Validation Epoch [68/200], Average Loss: 366.3708\n",
            "Training Epoch [69/200], Average Loss: 169.7234\n",
            "Validation Epoch [69/200], Average Loss: 364.8158\n",
            "Training Epoch [70/200], Average Loss: 166.8966\n",
            "Validation Epoch [70/200], Average Loss: 361.3414\n",
            "Training Epoch [71/200], Average Loss: 164.0517\n",
            "Validation Epoch [71/200], Average Loss: 358.6492\n",
            "Training Epoch [72/200], Average Loss: 161.6243\n",
            "Validation Epoch [72/200], Average Loss: 355.8807\n",
            "Training Epoch [73/200], Average Loss: 159.0769\n",
            "Validation Epoch [73/200], Average Loss: 354.8447\n",
            "Training Epoch [74/200], Average Loss: 156.7478\n",
            "Validation Epoch [74/200], Average Loss: 352.4438\n",
            "Training Epoch [75/200], Average Loss: 154.4094\n",
            "Validation Epoch [75/200], Average Loss: 350.1800\n",
            "Training Epoch [76/200], Average Loss: 151.9189\n",
            "Validation Epoch [76/200], Average Loss: 348.7530\n",
            "Training Epoch [77/200], Average Loss: 149.7027\n",
            "Validation Epoch [77/200], Average Loss: 346.4449\n",
            "Training Epoch [78/200], Average Loss: 147.4039\n",
            "Validation Epoch [78/200], Average Loss: 344.5988\n",
            "Training Epoch [79/200], Average Loss: 145.2486\n",
            "Validation Epoch [79/200], Average Loss: 343.3423\n",
            "Training Epoch [80/200], Average Loss: 143.0041\n",
            "Validation Epoch [80/200], Average Loss: 340.9546\n",
            "Training Epoch [81/200], Average Loss: 140.8163\n",
            "Validation Epoch [81/200], Average Loss: 337.9127\n",
            "Training Epoch [82/200], Average Loss: 138.7009\n",
            "Validation Epoch [82/200], Average Loss: 336.6084\n",
            "Training Epoch [83/200], Average Loss: 136.8866\n",
            "Validation Epoch [83/200], Average Loss: 335.0029\n",
            "Training Epoch [84/200], Average Loss: 135.3567\n",
            "Validation Epoch [84/200], Average Loss: 334.1617\n",
            "Training Epoch [85/200], Average Loss: 134.9368\n",
            "Validation Epoch [85/200], Average Loss: 336.2932\n",
            "Validation loss did not improve. Patience left: 4/5\n",
            "Training Epoch [86/200], Average Loss: 133.1223\n",
            "Validation Epoch [86/200], Average Loss: 332.6057\n",
            "Training Epoch [87/200], Average Loss: 130.1472\n",
            "Validation Epoch [87/200], Average Loss: 328.4341\n",
            "Training Epoch [88/200], Average Loss: 128.4072\n",
            "Validation Epoch [88/200], Average Loss: 326.8747\n",
            "Training Epoch [89/200], Average Loss: 127.8560\n",
            "Validation Epoch [89/200], Average Loss: 326.8072\n",
            "Training Epoch [90/200], Average Loss: 125.8922\n",
            "Validation Epoch [90/200], Average Loss: 329.0057\n",
            "Validation loss did not improve. Patience left: 4/5\n",
            "Training Epoch [91/200], Average Loss: 123.9125\n",
            "Validation Epoch [91/200], Average Loss: 323.2378\n",
            "Training Epoch [92/200], Average Loss: 121.6002\n",
            "Validation Epoch [92/200], Average Loss: 320.7869\n",
            "Training Epoch [93/200], Average Loss: 119.5976\n",
            "Validation Epoch [93/200], Average Loss: 320.4741\n",
            "Training Epoch [94/200], Average Loss: 117.8763\n",
            "Validation Epoch [94/200], Average Loss: 318.4670\n",
            "Training Epoch [95/200], Average Loss: 116.1459\n",
            "Validation Epoch [95/200], Average Loss: 316.5887\n",
            "Training Epoch [96/200], Average Loss: 114.4440\n",
            "Validation Epoch [96/200], Average Loss: 315.2465\n",
            "Training Epoch [97/200], Average Loss: 112.7455\n",
            "Validation Epoch [97/200], Average Loss: 314.5114\n",
            "Training Epoch [98/200], Average Loss: 111.5597\n",
            "Validation Epoch [98/200], Average Loss: 313.2781\n",
            "Training Epoch [99/200], Average Loss: 110.0340\n",
            "Validation Epoch [99/200], Average Loss: 311.0995\n",
            "Training Epoch [100/200], Average Loss: 108.5159\n",
            "Validation Epoch [100/200], Average Loss: 310.7917\n",
            "Training Epoch [101/200], Average Loss: 107.2060\n",
            "Validation Epoch [101/200], Average Loss: 309.2693\n",
            "Training Epoch [102/200], Average Loss: 105.7311\n",
            "Validation Epoch [102/200], Average Loss: 308.0757\n",
            "Training Epoch [103/200], Average Loss: 104.1553\n",
            "Validation Epoch [103/200], Average Loss: 306.2410\n",
            "Training Epoch [104/200], Average Loss: 103.1892\n",
            "Validation Epoch [104/200], Average Loss: 306.2331\n",
            "Training Epoch [105/200], Average Loss: 102.1177\n",
            "Validation Epoch [105/200], Average Loss: 304.9264\n",
            "Training Epoch [106/200], Average Loss: 100.6999\n",
            "Validation Epoch [106/200], Average Loss: 303.9692\n",
            "Training Epoch [107/200], Average Loss: 99.8733\n",
            "Validation Epoch [107/200], Average Loss: 303.2513\n",
            "Training Epoch [108/200], Average Loss: 98.5648\n",
            "Validation Epoch [108/200], Average Loss: 302.4585\n",
            "Training Epoch [109/200], Average Loss: 97.1842\n",
            "Validation Epoch [109/200], Average Loss: 301.1169\n",
            "Training Epoch [110/200], Average Loss: 95.8524\n",
            "Validation Epoch [110/200], Average Loss: 300.2321\n",
            "Training Epoch [111/200], Average Loss: 94.5686\n",
            "Validation Epoch [111/200], Average Loss: 299.7540\n",
            "Training Epoch [112/200], Average Loss: 93.4910\n",
            "Validation Epoch [112/200], Average Loss: 297.8768\n",
            "Training Epoch [113/200], Average Loss: 92.4676\n",
            "Validation Epoch [113/200], Average Loss: 296.6191\n",
            "Training Epoch [114/200], Average Loss: 91.1514\n",
            "Validation Epoch [114/200], Average Loss: 296.1773\n",
            "Training Epoch [115/200], Average Loss: 90.1843\n",
            "Validation Epoch [115/200], Average Loss: 295.0799\n",
            "Training Epoch [116/200], Average Loss: 89.1872\n",
            "Validation Epoch [116/200], Average Loss: 294.3031\n",
            "Training Epoch [117/200], Average Loss: 88.1308\n",
            "Validation Epoch [117/200], Average Loss: 293.6684\n",
            "Training Epoch [118/200], Average Loss: 87.1161\n",
            "Validation Epoch [118/200], Average Loss: 292.2633\n",
            "Training Epoch [119/200], Average Loss: 86.1029\n",
            "Validation Epoch [119/200], Average Loss: 291.8685\n",
            "Training Epoch [120/200], Average Loss: 85.1168\n",
            "Validation Epoch [120/200], Average Loss: 290.6672\n",
            "Training Epoch [121/200], Average Loss: 84.1392\n",
            "Validation Epoch [121/200], Average Loss: 290.7741\n",
            "Validation loss did not improve. Patience left: 4/5\n",
            "Training Epoch [122/200], Average Loss: 83.2774\n",
            "Validation Epoch [122/200], Average Loss: 288.6975\n",
            "Training Epoch [123/200], Average Loss: 82.3796\n",
            "Validation Epoch [123/200], Average Loss: 289.2701\n",
            "Validation loss did not improve. Patience left: 4/5\n",
            "Training Epoch [124/200], Average Loss: 81.3900\n",
            "Validation Epoch [124/200], Average Loss: 287.4021\n",
            "Training Epoch [125/200], Average Loss: 80.3445\n",
            "Validation Epoch [125/200], Average Loss: 286.2728\n",
            "Training Epoch [126/200], Average Loss: 79.5798\n",
            "Validation Epoch [126/200], Average Loss: 285.9163\n",
            "Training Epoch [127/200], Average Loss: 78.6043\n",
            "Validation Epoch [127/200], Average Loss: 285.4663\n",
            "Training Epoch [128/200], Average Loss: 77.7565\n",
            "Validation Epoch [128/200], Average Loss: 285.3719\n",
            "Training Epoch [129/200], Average Loss: 77.1086\n",
            "Validation Epoch [129/200], Average Loss: 285.2279\n",
            "Training Epoch [130/200], Average Loss: 76.3491\n",
            "Validation Epoch [130/200], Average Loss: 284.4052\n",
            "Training Epoch [131/200], Average Loss: 75.7232\n",
            "Validation Epoch [131/200], Average Loss: 283.6123\n",
            "Training Epoch [132/200], Average Loss: 74.8522\n",
            "Validation Epoch [132/200], Average Loss: 281.6460\n",
            "Training Epoch [133/200], Average Loss: 73.8289\n",
            "Validation Epoch [133/200], Average Loss: 281.8090\n",
            "Validation loss did not improve. Patience left: 4/5\n",
            "Training Epoch [134/200], Average Loss: 73.3125\n",
            "Validation Epoch [134/200], Average Loss: 281.5370\n",
            "Training Epoch [135/200], Average Loss: 72.5293\n",
            "Validation Epoch [135/200], Average Loss: 279.7042\n",
            "Training Epoch [136/200], Average Loss: 71.5453\n",
            "Validation Epoch [136/200], Average Loss: 279.4440\n",
            "Training Epoch [137/200], Average Loss: 70.8115\n",
            "Validation Epoch [137/200], Average Loss: 278.3470\n",
            "Training Epoch [138/200], Average Loss: 69.8402\n",
            "Validation Epoch [138/200], Average Loss: 277.9791\n",
            "Training Epoch [139/200], Average Loss: 69.1486\n",
            "Validation Epoch [139/200], Average Loss: 278.3529\n",
            "Validation loss did not improve. Patience left: 4/5\n",
            "Training Epoch [140/200], Average Loss: 68.6209\n",
            "Validation Epoch [140/200], Average Loss: 277.5599\n",
            "Training Epoch [141/200], Average Loss: 68.1359\n",
            "Validation Epoch [141/200], Average Loss: 275.9234\n",
            "Training Epoch [142/200], Average Loss: 67.2568\n",
            "Validation Epoch [142/200], Average Loss: 276.8452\n",
            "Validation loss did not improve. Patience left: 4/5\n",
            "Training Epoch [143/200], Average Loss: 66.7524\n",
            "Validation Epoch [143/200], Average Loss: 275.5023\n",
            "Training Epoch [144/200], Average Loss: 65.8433\n",
            "Validation Epoch [144/200], Average Loss: 275.8187\n",
            "Validation loss did not improve. Patience left: 4/5\n",
            "Training Epoch [145/200], Average Loss: 65.3805\n",
            "Validation Epoch [145/200], Average Loss: 275.7747\n",
            "Validation loss did not improve. Patience left: 3/5\n",
            "Training Epoch [146/200], Average Loss: 64.6931\n",
            "Validation Epoch [146/200], Average Loss: 275.0183\n",
            "Training Epoch [147/200], Average Loss: 63.9547\n",
            "Validation Epoch [147/200], Average Loss: 273.9629\n",
            "Training Epoch [148/200], Average Loss: 63.1765\n",
            "Validation Epoch [148/200], Average Loss: 272.7422\n",
            "Training Epoch [149/200], Average Loss: 62.5049\n",
            "Validation Epoch [149/200], Average Loss: 272.1876\n",
            "Training Epoch [150/200], Average Loss: 61.7107\n",
            "Validation Epoch [150/200], Average Loss: 272.0988\n",
            "Training Epoch [151/200], Average Loss: 61.1048\n",
            "Validation Epoch [151/200], Average Loss: 271.6679\n",
            "Training Epoch [152/200], Average Loss: 60.6724\n",
            "Validation Epoch [152/200], Average Loss: 270.6269\n",
            "Training Epoch [153/200], Average Loss: 60.0573\n",
            "Validation Epoch [153/200], Average Loss: 270.3761\n",
            "Training Epoch [154/200], Average Loss: 59.3704\n",
            "Validation Epoch [154/200], Average Loss: 270.3791\n",
            "Validation loss did not improve. Patience left: 4/5\n",
            "Training Epoch [155/200], Average Loss: 58.8757\n",
            "Validation Epoch [155/200], Average Loss: 270.5924\n",
            "Validation loss did not improve. Patience left: 3/5\n",
            "Training Epoch [156/200], Average Loss: 58.2183\n",
            "Validation Epoch [156/200], Average Loss: 270.0751\n",
            "Training Epoch [157/200], Average Loss: 57.5547\n",
            "Validation Epoch [157/200], Average Loss: 268.3490\n",
            "Training Epoch [158/200], Average Loss: 57.1702\n",
            "Validation Epoch [158/200], Average Loss: 268.3543\n",
            "Validation loss did not improve. Patience left: 4/5\n",
            "Training Epoch [159/200], Average Loss: 56.5265\n",
            "Validation Epoch [159/200], Average Loss: 268.0116\n",
            "Training Epoch [160/200], Average Loss: 56.1749\n",
            "Validation Epoch [160/200], Average Loss: 268.1788\n",
            "Validation loss did not improve. Patience left: 4/5\n",
            "Training Epoch [161/200], Average Loss: 55.5205\n",
            "Validation Epoch [161/200], Average Loss: 267.0078\n",
            "Training Epoch [162/200], Average Loss: 54.9127\n",
            "Validation Epoch [162/200], Average Loss: 266.4852\n",
            "Training Epoch [163/200], Average Loss: 54.4789\n",
            "Validation Epoch [163/200], Average Loss: 266.4678\n",
            "Training Epoch [164/200], Average Loss: 54.0419\n",
            "Validation Epoch [164/200], Average Loss: 266.7458\n",
            "Validation loss did not improve. Patience left: 4/5\n",
            "Training Epoch [165/200], Average Loss: 53.4430\n",
            "Validation Epoch [165/200], Average Loss: 265.6688\n",
            "Training Epoch [166/200], Average Loss: 53.1739\n",
            "Validation Epoch [166/200], Average Loss: 265.3235\n",
            "Training Epoch [167/200], Average Loss: 52.5912\n",
            "Validation Epoch [167/200], Average Loss: 264.5679\n",
            "Training Epoch [168/200], Average Loss: 51.9484\n",
            "Validation Epoch [168/200], Average Loss: 264.1182\n",
            "Training Epoch [169/200], Average Loss: 51.4822\n",
            "Validation Epoch [169/200], Average Loss: 263.7193\n",
            "Training Epoch [170/200], Average Loss: 50.9395\n",
            "Validation Epoch [170/200], Average Loss: 263.4719\n",
            "Training Epoch [171/200], Average Loss: 50.5179\n",
            "Validation Epoch [171/200], Average Loss: 263.1018\n",
            "Training Epoch [172/200], Average Loss: 49.9481\n",
            "Validation Epoch [172/200], Average Loss: 262.8368\n",
            "Training Epoch [173/200], Average Loss: 49.4521\n",
            "Validation Epoch [173/200], Average Loss: 263.4091\n",
            "Validation loss did not improve. Patience left: 4/5\n",
            "Training Epoch [174/200], Average Loss: 49.1201\n",
            "Validation Epoch [174/200], Average Loss: 262.5193\n",
            "Training Epoch [175/200], Average Loss: 48.5940\n",
            "Validation Epoch [175/200], Average Loss: 262.3925\n",
            "Training Epoch [176/200], Average Loss: 48.3678\n",
            "Validation Epoch [176/200], Average Loss: 262.3235\n",
            "Training Epoch [177/200], Average Loss: 48.5463\n",
            "Validation Epoch [177/200], Average Loss: 261.7398\n",
            "Training Epoch [178/200], Average Loss: 47.7408\n",
            "Validation Epoch [178/200], Average Loss: 260.9901\n",
            "Training Epoch [179/200], Average Loss: 47.1614\n",
            "Validation Epoch [179/200], Average Loss: 260.1186\n",
            "Training Epoch [180/200], Average Loss: 46.5619\n",
            "Validation Epoch [180/200], Average Loss: 260.8548\n",
            "Validation loss did not improve. Patience left: 4/5\n",
            "Training Epoch [181/200], Average Loss: 46.2847\n",
            "Validation Epoch [181/200], Average Loss: 261.1946\n",
            "Validation loss did not improve. Patience left: 3/5\n",
            "Training Epoch [182/200], Average Loss: 45.9761\n",
            "Validation Epoch [182/200], Average Loss: 259.5803\n",
            "Training Epoch [183/200], Average Loss: 45.5030\n",
            "Validation Epoch [183/200], Average Loss: 259.9500\n",
            "Validation loss did not improve. Patience left: 4/5\n",
            "Training Epoch [184/200], Average Loss: 45.0156\n",
            "Validation Epoch [184/200], Average Loss: 258.9751\n",
            "Training Epoch [185/200], Average Loss: 44.7573\n",
            "Validation Epoch [185/200], Average Loss: 258.7860\n",
            "Training Epoch [186/200], Average Loss: 44.2760\n",
            "Validation Epoch [186/200], Average Loss: 258.5347\n",
            "Training Epoch [187/200], Average Loss: 43.9898\n",
            "Validation Epoch [187/200], Average Loss: 259.7068\n",
            "Validation loss did not improve. Patience left: 4/5\n",
            "Training Epoch [188/200], Average Loss: 43.6236\n",
            "Validation Epoch [188/200], Average Loss: 258.0277\n",
            "Training Epoch [189/200], Average Loss: 43.0932\n",
            "Validation Epoch [189/200], Average Loss: 258.0228\n",
            "Training Epoch [190/200], Average Loss: 42.6800\n",
            "Validation Epoch [190/200], Average Loss: 258.1956\n",
            "Validation loss did not improve. Patience left: 4/5\n",
            "Training Epoch [191/200], Average Loss: 42.3160\n",
            "Validation Epoch [191/200], Average Loss: 257.5406\n",
            "Training Epoch [192/200], Average Loss: 41.8688\n",
            "Validation Epoch [192/200], Average Loss: 257.7301\n",
            "Validation loss did not improve. Patience left: 4/5\n",
            "Training Epoch [193/200], Average Loss: 41.6319\n",
            "Validation Epoch [193/200], Average Loss: 257.7829\n",
            "Validation loss did not improve. Patience left: 3/5\n",
            "Training Epoch [194/200], Average Loss: 41.3793\n",
            "Validation Epoch [194/200], Average Loss: 256.9295\n",
            "Training Epoch [195/200], Average Loss: 41.0827\n",
            "Validation Epoch [195/200], Average Loss: 256.7887\n",
            "Training Epoch [196/200], Average Loss: 40.6651\n",
            "Validation Epoch [196/200], Average Loss: 256.7603\n",
            "Training Epoch [197/200], Average Loss: 40.2410\n",
            "Validation Epoch [197/200], Average Loss: 255.9205\n",
            "Training Epoch [198/200], Average Loss: 39.8029\n",
            "Validation Epoch [198/200], Average Loss: 255.4347\n",
            "Training Epoch [199/200], Average Loss: 39.4804\n",
            "Validation Epoch [199/200], Average Loss: 255.7048\n",
            "Validation loss did not improve. Patience left: 4/5\n",
            "Training Epoch [200/200], Average Loss: 39.2299\n",
            "Validation Epoch [200/200], Average Loss: 255.5003\n",
            "Validation loss did not improve. Patience left: 3/5\n",
            "Training finished!\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model = SimpleNN(input_size, output_size)\n",
        "\n",
        "# Check if GPU is available\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move model to the GPU\n",
        "model.to(device)\n",
        "\n",
        "# Move the criterion and optimizer to the GPU\n",
        "criterion = nn.MSELoss().to(device)\n",
        "#optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=0.001)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.0001)\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "# Early stopping parameters\n",
        "early_stopping_patience = 5\n",
        "best_val_loss = float('inf')\n",
        "counter = 0\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 200\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_train_loss = 0.0\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        # Move inputs and targets to GPU\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        # Flatten the inputs if needed\n",
        "        inputs = inputs.view(inputs.size(0), -1)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate the loss\n",
        "        loss = criterion(outputs, targets)\n",
        "\n",
        "        # L2 regularization term\n",
        "        #l2_reg = torch.tensor(0.).to(device)\n",
        "        #for param in model.parameters():\n",
        "        #    l2_reg += torch.norm(param, p=2)\n",
        "\n",
        "        #loss += 0.001 * l2_reg\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_train_loss += loss.item()\n",
        "\n",
        "    # Print the average training loss for the epoch\n",
        "    average_train_loss = running_train_loss / len(train_loader)\n",
        "    print(f'Training Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_train_loss:.4f}')\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    running_val_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (val_inputs, val_targets) in enumerate(val_loader):\n",
        "            # Move validation inputs and targets to GPU\n",
        "            val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
        "            val_inputs = val_inputs.view(val_inputs.size(0), -1)\n",
        "\n",
        "            val_outputs = model(val_inputs)\n",
        "            val_loss = criterion(val_outputs, val_targets)\n",
        "            running_val_loss += val_loss.item()\n",
        "\n",
        "    # Print the average validation loss for the epoch\n",
        "    average_val_loss = running_val_loss / len(val_loader)\n",
        "    print(f'Validation Epoch [{epoch + 1}/{num_epochs}], Average Loss: {average_val_loss:.4f}')\n",
        "\n",
        "    # Early stopping check\n",
        "    if average_val_loss < best_val_loss:\n",
        "        best_val_loss = average_val_loss\n",
        "        counter = 0\n",
        "    else:\n",
        "        counter += 1\n",
        "        print(f'Validation loss did not improve. Patience left: {early_stopping_patience - counter}/{early_stopping_patience}')\n",
        "        if counter >= early_stopping_patience:\n",
        "            print(f'Early stopping after {epoch + 1 - early_stopping_patience} epochs without improvement.')\n",
        "            break\n",
        "\n",
        "print('Training finished!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vomadUYIUqVH"
      },
      "outputs": [],
      "source": [
        "torch.save(model,'parrot.pth')\n",
        "\n",
        "checkpoint = {\n",
        "    'epoch': epoch + 1,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': average_val_loss,\n",
        "    'model_architecture': SimpleNN(input_size, output_size)\n",
        "}\n",
        "torch.save(checkpoint, 'parrot.pth')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "from torchvision import transforms\n",
        "\n",
        "temp = transposed_data2.iloc[1:1, :12288]\n",
        "tensor = torch.tensor(temp.values, dtype=torch.float32)\n",
        "\n",
        "img = Image.open(\"2.jpg\")\n",
        "\n",
        "img = img.resize((64,64))\n",
        "img.save(\"2.jpg\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "img_tensor = transform(img).unsqueeze(0)\n",
        "img_tensor = img_tensor.view(1, -1)\n",
        "print(img_tensor.shape)\n",
        "\n",
        "input_size = 64 *64*3  # Replace with the actual input size\n",
        "output_size = 64*64  # Replace with the actual output size\n",
        "\n",
        "# Load the model architecture and state dictionary\n",
        "checkpoint = torch.load('parrot.pth')\n",
        "model_architecture = SimpleNN(input_size, output_size)\n",
        "model_architecture.load_state_dict(checkpoint['model_state_dict'])\n",
        "model_architecture.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model_architecture(img_tensor)\n",
        "\n",
        "output = output.view(1, 64, 64)\n",
        "output = Image.fromarray(output.squeeze().numpy().astype('uint8'))\n",
        "\n",
        "output.save(\"2_out.jpg\")\n"
      ],
      "metadata": {
        "id": "nz2_ewvnnxMM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "701c3f19-611f-4954-ad3e-98c161ba5064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 12288])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9RFj8xJYg1ss"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}